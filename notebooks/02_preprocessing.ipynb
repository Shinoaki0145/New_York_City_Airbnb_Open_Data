{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fa4530",
   "metadata": {},
   "source": [
    "# Airbnb NYC 2019 Data Preprocessing\n",
    "\n",
    "This notebook focuses on preparing the data for machine learning models. The main objectives are:\n",
    "\n",
    "1. **Data Cleaning**:\n",
    "   - Handle missing values and outliers.\n",
    "   - Correct data types and inconsistencies.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Create new features from existing ones.\n",
    "   - Encode categorical variables.\n",
    "   - Scale numerical features.\n",
    "\n",
    "3. **Data Splitting**:\n",
    "   - Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae7833",
   "metadata": {},
   "source": [
    "## 1. Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1584b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "from data_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa62c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Initial shape: (48895, 16)\n",
      "Initial number of columns: 16\n"
     ]
    }
   ],
   "source": [
    "data_numpy = read_csv('../data/raw/AB_NYC_2019.csv')\n",
    "\n",
    "column_names = data_numpy[0]\n",
    "data = data_numpy[1:]\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "print(f\"Initial shape: {data.shape}\")\n",
    "print(f\"Initial number of columns: {len(column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed184fd",
   "metadata": {},
   "source": [
    "## 2. Check initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d5f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial column list:\n",
      "  1. id\n",
      "  2. name\n",
      "  3. host_id\n",
      "  4. host_name\n",
      "  5. neighbourhood_group\n",
      "  6. neighbourhood\n",
      "  7. latitude\n",
      "  8. longitude\n",
      "  9. room_type\n",
      "  10. price\n",
      "  11. minimum_nights\n",
      "  12. number_of_reviews\n",
      "  13. last_review\n",
      "  14. reviews_per_month\n",
      "  15. calculated_host_listings_count\n",
      "  16. availability_365\n",
      "\n",
      "==================================================\n",
      "Missing values in initial data:\n",
      "\n",
      "name:\n",
      "  - Count: 16\n",
      "  - Percentage: 0.033%\n",
      "\n",
      "host_name:\n",
      "  - Count: 21\n",
      "  - Percentage: 0.043%\n",
      "\n",
      "last_review:\n",
      "  - Count: 10,052\n",
      "  - Percentage: 20.558%\n",
      "\n",
      "reviews_per_month:\n",
      "  - Count: 10,052\n",
      "  - Percentage: 20.558%\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial column list:\")\n",
    "for i, col in enumerate(column_names):\n",
    "    print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Missing values in initial data:\")\n",
    "for i, col in enumerate(column_names):\n",
    "    col_data = data[:, i]\n",
    "    missing_count = np.sum(col_data == '')\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        missing_percent = (missing_count / len(data) * 100)\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  - Count: {missing_count:,}\")\n",
    "        print(f\"  - Percentage: {missing_percent:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297f47f",
   "metadata": {},
   "source": [
    "## 3. Remove unnecessary columns\n",
    "\n",
    "**Columns to drop:**\n",
    "- `id`: No value for calculation/analysis purposes\n",
    "- `name`: Just a descriptive name, no numerical value\n",
    "- `host_name`: \n",
    "  - Has 21 missing values\n",
    "  - Already have `host_id` with more complete data\n",
    "- `last_review`: \n",
    "  - Not as important as review count\n",
    "  - Already have `number_of_reviews` and `reviews_per_month` for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161e6164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before drop: (48895, 16)\n",
      "Shape after drop: (48895, 12)\n",
      "\n",
      "Dropped columns: ['id', 'name', 'host_name', 'last_review']\n",
      "\n",
      "Remaining columns (12):\n",
      "1. host_id\n",
      "2. neighbourhood_group\n",
      "3. neighbourhood\n",
      "4. latitude\n",
      "5. longitude\n",
      "6. room_type\n",
      "7. price\n",
      "8. minimum_nights\n",
      "9. number_of_reviews\n",
      "10. reviews_per_month\n",
      "11. calculated_host_listings_count\n",
      "12. availability_365\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['id', 'name', 'host_name', 'last_review']\n",
    "\n",
    "is_drop_col = np.isin(column_names, columns_to_drop)\n",
    "\n",
    "keep_mask = ~is_drop_col\n",
    "\n",
    "data_cleaned = data[:, keep_mask]\n",
    "column_names_cleaned = column_names[keep_mask]\n",
    "\n",
    "print(f\"Shape before drop: {data.shape}\")\n",
    "print(f\"Shape after drop: {data_cleaned.shape}\")\n",
    "print(f\"\\nDropped columns: {columns_to_drop}\")\n",
    "\n",
    "print(f\"\\nRemaining columns ({len(column_names_cleaned)}):\")\n",
    "print(\"\\n\".join([f\"{i+1}. {col}\" for i, col in enumerate(column_names_cleaned)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3874cb",
   "metadata": {},
   "source": [
    "## 4. Handle missing values in reviews_per_month\n",
    "\n",
    "**Reason for assigning 0.00:**\n",
    "- Column `reviews_per_month` has 10,052 missing values\n",
    "- When cross-checking with `number_of_reviews`, listings with missing `reviews_per_month` all have `number_of_reviews = 0`\n",
    "- Logic: If there are no reviews, then average reviews/month = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af0d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before processing: 10,052\n",
      "Missing values after processing: 0\n",
      "\n",
      "Replaced 10,052 missing values with 0.00\n"
     ]
    }
   ],
   "source": [
    "reviews_per_month_idx = np.where(column_names_cleaned == 'reviews_per_month')[0][0]\n",
    "\n",
    "mask_missing = (data_cleaned[:, reviews_per_month_idx] == '')\n",
    "\n",
    "missing_before = np.sum(mask_missing)\n",
    "print(f\"Missing values before processing: {missing_before:,}\")\n",
    "\n",
    "data_cleaned[mask_missing, reviews_per_month_idx] = '0.00'\n",
    "missing_after = np.sum(data_cleaned[:, reviews_per_month_idx] == '')\n",
    "\n",
    "print(f\"Missing values after processing: {missing_after:,}\")\n",
    "print(f\"\\nReplaced {missing_before:,} missing values with 0.00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067aa64",
   "metadata": {},
   "source": [
    "## 5. Convert data to appropriate types\n",
    "\n",
    "**Purpose:** Ensure consistent and optimized data types for computation\n",
    "- String columns: `astype(str)` \n",
    "- Numeric columns: `astype(np.float64)` for high precision\n",
    "\n",
    "**Technique:** Use `pandas.DataFrame.astype()` to efficiently and quickly convert column data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5f40ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data processing completed!\n"
     ]
    }
   ],
   "source": [
    "neighbourhood_group_idx = np.where(column_names_cleaned == 'neighbourhood_group')[0][0]\n",
    "neighbourhood_idx = np.where(column_names_cleaned == 'neighbourhood')[0][0]\n",
    "room_type_idx = np.where(column_names_cleaned == 'room_type')[0][0]\n",
    "latitude_idx = np.where(column_names_cleaned == 'latitude')[0][0]\n",
    "longitude_idx = np.where(column_names_cleaned == 'longitude')[0][0]\n",
    "price_idx = np.where(column_names_cleaned == 'price')[0][0]\n",
    "min_nights_idx = np.where(column_names_cleaned == 'minimum_nights')[0][0]\n",
    "num_reviews_idx = np.where(column_names_cleaned == 'number_of_reviews')[0][0]\n",
    "reviews_per_month_idx = np.where(column_names_cleaned == 'reviews_per_month')[0][0]\n",
    "calc_host_idx = np.where(column_names_cleaned == 'calculated_host_listings_count')[0][0]\n",
    "availability_idx = np.where(column_names_cleaned == 'availability_365')[0][0]\n",
    "\n",
    "neighbourhood_groups = data_cleaned[:, neighbourhood_group_idx]\n",
    "neighbourhoods = data_cleaned[:, neighbourhood_idx]\n",
    "room_types = data_cleaned[:, room_type_idx]\n",
    "\n",
    "latitudes = data_cleaned[:, latitude_idx].astype(float)\n",
    "longitudes = data_cleaned[:, longitude_idx].astype(float)\n",
    "prices = data_cleaned[:, price_idx].astype(float)\n",
    "min_nights = data_cleaned[:, min_nights_idx].astype(float)\n",
    "number_of_reviews = data_cleaned[:, num_reviews_idx].astype(float)\n",
    "calc_host_listings = data_cleaned[:, calc_host_idx].astype(float)\n",
    "reviews_per_month = data_cleaned[:, reviews_per_month_idx].astype(float)\n",
    "availability = data_cleaned[:, availability_idx].astype(float)\n",
    "\n",
    "print(\"\\nData processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd830cb4",
   "metadata": {},
   "source": [
    "## 6. Filter noisy data\n",
    "\n",
    "**Purpose:** Remove records with invalid values to ensure data quality\n",
    "- Focus on `price` column\n",
    "\n",
    "**Technique:** Use Boolean Masking - Fancy Indexing to filter invalid data (price < 0)\n",
    "- No loops needed\n",
    "- Memory efficient (views instead of copies)\n",
    "- Clean and readable syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "377df92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before filtering: 48,895\n",
      "Rows with price <= 0: 11\n",
      "Rows after filtering: 48,884\n",
      "\n",
      "Filtering successful, 48,884 valid rows remaining\n"
     ]
    }
   ],
   "source": [
    "valid_mask = prices > 0 \n",
    "print(f\"Rows before filtering: {len(prices):,}\")\n",
    "print(f\"Rows with price <= 0: {np.sum(~valid_mask):,}\")\n",
    "print(f\"Rows after filtering: {np.sum(valid_mask):,}\")\n",
    "\n",
    "prices = prices[valid_mask]\n",
    "min_nights = min_nights[valid_mask]\n",
    "latitudes = latitudes[valid_mask]\n",
    "longitudes = longitudes[valid_mask]\n",
    "neighbourhood_groups = neighbourhood_groups[valid_mask]\n",
    "neighbourhoods = neighbourhoods[valid_mask]\n",
    "room_types = room_types[valid_mask]\n",
    "number_of_reviews = number_of_reviews[valid_mask]\n",
    "reviews_per_month = reviews_per_month[valid_mask]\n",
    "calc_host_listings = calc_host_listings[valid_mask]\n",
    "availability = availability[valid_mask]\n",
    "\n",
    "print(f\"\\nFiltering successful, {len(prices):,} valid rows remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912e3de",
   "metadata": {},
   "source": [
    "## 7. One-hot encoding for neighbourhood_group column\n",
    "\n",
    "**Purpose:** Convert categorical variables `neighbourhood_group` and `room_type` into numeric form for use in machine learning models\n",
    "- Create separate binary columns for each value in `neighbourhood_group` and `room_type`\n",
    "\n",
    "**Technique:** One-hot encoding using broadcasting to convert `neighbourhood_group` and `room_type` columns into binary columns\n",
    "- Broadcasting: `(N,1) == (C,)` → `(N,C)` \n",
    "- Speed: O(N×C) but vectorized so much faster than loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ec1874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding Results:\n",
      "- neighbourhood_group: (48884, 5)\n",
      "  Classes: ['Bronx' 'Brooklyn' 'Manhattan' 'Queens' 'Staten Island']\n",
      "\n",
      "- room_type: (48884, 3)\n",
      "  Classes: ['Entire home/apt' 'Private room' 'Shared room']\n",
      "\n",
      "Sample one-hot encoding for room_type (first 5 rows):\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def fast_one_hot(arr):\n",
    "    \"\"\"\n",
    "    One-hot encoding using Broadcasting\n",
    "    arr: (N,) -> output: (N, C) where C is number of unique classes\n",
    "    \"\"\"\n",
    "    classes = np.unique(arr)\n",
    "    # Broadcasting: arr[:, None] shape (N,1), classes shape (C,)\n",
    "    # Comparison result: (N,1) == (C,) -> (N,C)\n",
    "    return (arr[:, None] == classes).astype(int)\n",
    "\n",
    "# Apply one-hot encoding\n",
    "oh_neighbourhood_group = fast_one_hot(neighbourhood_groups)\n",
    "oh_room_type = fast_one_hot(room_types)\n",
    "\n",
    "print(\"One-Hot Encoding Results:\")\n",
    "print(f\"- neighbourhood_group: {oh_neighbourhood_group.shape}\")\n",
    "print(f\"  Classes: {np.unique(neighbourhood_groups)}\")\n",
    "print(f\"\\n- room_type: {oh_room_type.shape}\")\n",
    "print(f\"  Classes: {np.unique(room_types)}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nSample one-hot encoding for room_type (first 5 rows):\")\n",
    "print(oh_room_type[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5084ea8",
   "metadata": {},
   "source": [
    "## 8. Splitting the train and test sets\n",
    "\n",
    "**Purpose:** Split the data into train (80%) and test (20%) sets before performing Feature Engineering\n",
    "- Avoid data leakage when applying target encoding and scaling\n",
    "- Ensure the test set is completely independent from the train set\n",
    "\n",
    "**Technique:** Random sampling with np.random.permutation\n",
    "- Shuffle random data\n",
    "- Split in an 80-20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ccd3518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into train and test:\n",
      "- Total samples: 48,884\n",
      "- Train set: 39,107 samples (80%)\n",
      "- Test set: 9,777 samples (20%)\n",
      "\n",
      "Shape check:\n",
      "- prices_train: (39107,)\n",
      "- prices_test: (9777,)\n",
      "- neighbourhoods_train: (39107,)\n",
      "- neighbourhoods_test: (9777,)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Total number of samples\n",
    "n_samples = len(prices)\n",
    "\n",
    "# Create random indices\n",
    "shuffled_indices = np.random.permutation(n_samples)\n",
    "\n",
    "# Calculate train size (80%)\n",
    "train_size = int(0.8 * n_samples)\n",
    "\n",
    "# Split indices\n",
    "train_indices = shuffled_indices[:train_size]\n",
    "test_indices = shuffled_indices[train_size:]\n",
    "\n",
    "# Split arrays into train and test\n",
    "# Numerical features\n",
    "prices_train, prices_test = prices[train_indices], prices[test_indices]\n",
    "latitudes_train, latitudes_test = latitudes[train_indices], latitudes[test_indices]\n",
    "longitudes_train, longitudes_test = longitudes[train_indices], longitudes[test_indices]\n",
    "min_nights_train, min_nights_test = min_nights[train_indices], min_nights[test_indices]\n",
    "number_of_reviews_train, number_of_reviews_test = number_of_reviews[train_indices], number_of_reviews[test_indices]\n",
    "reviews_per_month_train, reviews_per_month_test = reviews_per_month[train_indices], reviews_per_month[test_indices]\n",
    "calc_host_listings_train, calc_host_listings_test = calc_host_listings[train_indices], calc_host_listings[test_indices]\n",
    "availability_train, availability_test = availability[train_indices], availability[test_indices]\n",
    "\n",
    "# Categorical features\n",
    "neighbourhood_groups_train, neighbourhood_groups_test = neighbourhood_groups[train_indices], neighbourhood_groups[test_indices]\n",
    "neighbourhoods_train, neighbourhoods_test = neighbourhoods[train_indices], neighbourhoods[test_indices]\n",
    "room_types_train, room_types_test = room_types[train_indices], room_types[test_indices]\n",
    "\n",
    "# One-hot encoded features (created in step 7)\n",
    "oh_neighbourhood_group_train, oh_neighbourhood_group_test = oh_neighbourhood_group[train_indices], oh_neighbourhood_group[test_indices]\n",
    "oh_room_type_train, oh_room_type_test = oh_room_type[train_indices], oh_room_type[test_indices]\n",
    "\n",
    "print(\"Split data into train and test:\")\n",
    "print(f\"- Total samples: {n_samples:,}\")\n",
    "print(f\"- Train set: {len(train_indices):,} samples ({len(train_indices)/n_samples*100:.0f}%)\")\n",
    "print(f\"- Test set: {len(test_indices):,} samples ({len(test_indices)/n_samples*100:.0f}%)\")\n",
    "print(f\"\\nShape check:\")\n",
    "print(f\"- prices_train: {prices_train.shape}\")\n",
    "print(f\"- prices_test: {prices_test.shape}\")\n",
    "print(f\"- neighbourhoods_train: {neighbourhoods_train.shape}\")\n",
    "print(f\"- neighbourhoods_test: {neighbourhoods_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f4caa",
   "metadata": {},
   "source": [
    "## 9. Feature Engineering - Create new features\n",
    "\n",
    "**Purpose:** Create new variables from original data to improve machine learning model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af95cb",
   "metadata": {},
   "source": [
    "### 9.1. Calculate distance to Times Square\n",
    "\n",
    "**Purpose:** Calculate distance from each listing to Times Square to use as a feature in machine learning models\n",
    "- Times Square coordinates: (40.7580, -73.9855)\n",
    "\n",
    "**Technique:** Use `np.einsum` for linear algebra\n",
    "- Calculate distance from each listing to Times Square (40.7580, -73.9855)\n",
    "- `einsum('ij,ij->i')`: Element-wise multiplication then sum along axis 1\n",
    "- Much more efficient than loops or `np.sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3de0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to Times Square:\n",
      "\n",
      "Train set:\n",
      "  - Min: 0.000677 degrees\n",
      "  - Max: 0.359430 degrees\n",
      "  - Mean: 0.070638 degrees\n",
      "\n",
      "Test set:\n",
      "  - Min: 0.000744 degrees\n",
      "  - Max: 0.363140 degrees\n",
      "  - Mean: 0.069868 degrees\n"
     ]
    }
   ],
   "source": [
    "# Times Square coordinates\n",
    "center = np.array([40.7580, -73.9855])\n",
    "\n",
    "# Calculate distance for train set\n",
    "coords_train = np.column_stack((latitudes_train, longitudes_train))\n",
    "diff_train = coords_train - center\n",
    "dist_sq_train = np.einsum('ij,ij->i', diff_train, diff_train)\n",
    "dist_to_center_train = np.sqrt(dist_sq_train)\n",
    "\n",
    "# Calculate distance for test set\n",
    "coords_test = np.column_stack((latitudes_test, longitudes_test))\n",
    "diff_test = coords_test - center\n",
    "dist_sq_test = np.einsum('ij,ij->i', diff_test, diff_test)\n",
    "dist_to_center_test = np.sqrt(dist_sq_test)\n",
    "\n",
    "print(\"Distance to Times Square:\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  - Min: {dist_to_center_train.min():.6f} degrees\")\n",
    "print(f\"  - Max: {dist_to_center_train.max():.6f} degrees\")\n",
    "print(f\"  - Mean: {dist_to_center_train.mean():.6f} degrees\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - Min: {dist_to_center_test.min():.6f} degrees\")\n",
    "print(f\"  - Max: {dist_to_center_test.max():.6f} degrees\")\n",
    "print(f\"  - Mean: {dist_to_center_test.mean():.6f} degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341f292",
   "metadata": {},
   "source": [
    "### 9.2 Binning for minimum_nights\n",
    "\n",
    "**Purpose:** \n",
    "- Remove the influence of extreme outliers (e.g., 1250 nights) which can skew distance-based algorithms.\n",
    "- Categorize minimum_nights into meaningful groups\n",
    "    - Bin 0: ≤3 nights (short-term travelers)\n",
    "    - Bin 1: 4-7 nights (weekly rental)\n",
    "    - Bin 2: 8-31 nights (monthly rental)\n",
    "    - Bin 3: >31 nights (long-term)\n",
    "\n",
    "**Technique:** \n",
    "- `np.digitize(arr, bins, right=True)`: Categorize values into bins\n",
    "- `np.eye(num_categories)[indices]`: Create one-hot encoding from indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edaf6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning minimum_nights:\n",
      "\n",
      "Train set - oh_min_nights shape: (39107, 4)\n",
      "Distribution:\n",
      "  Bin 0 (≤3 nights (short-term)): 25960 listings (66.382%)\n",
      "  Bin 1 (4-7 nights (weekly)): 7334 listings (18.754%)\n",
      "  Bin 2 (8-31 nights (monthly)): 5374 listings (13.742%)\n",
      "  Bin 3 (>31 nights (long-term)): 439 listings (1.123%)\n",
      "\n",
      "Test set - oh_min_nights shape: (9777, 4)\n",
      "Distribution:\n",
      "  Bin 0 (≤3 nights (short-term)): 6448 listings (65.951%)\n",
      "  Bin 1 (4-7 nights (weekly)): 1811 listings (18.523%)\n",
      "  Bin 2 (8-31 nights (monthly)): 1411 listings (14.432%)\n",
      "  Bin 3 (>31 nights (long-term)): 107 listings (1.094%)\n"
     ]
    }
   ],
   "source": [
    "# Define bin boundaries\n",
    "bins = np.array([3, 7, 31])\n",
    "\n",
    "# Binning for train set\n",
    "min_nights_binned_indices_train = np.digitize(min_nights_train, bins, right=True)\n",
    "num_categories = 4\n",
    "oh_min_nights_train = np.eye(num_categories)[min_nights_binned_indices_train]\n",
    "\n",
    "# Binning for test set\n",
    "min_nights_binned_indices_test = np.digitize(min_nights_test, bins, right=True)\n",
    "oh_min_nights_test = np.eye(num_categories)[min_nights_binned_indices_test]\n",
    "\n",
    "print(\"Binning minimum_nights:\")\n",
    "print(f\"\\nTrain set - oh_min_nights shape: {oh_min_nights_train.shape}\")\n",
    "print(\"Distribution:\")\n",
    "for i in range(num_categories):\n",
    "    count = np.sum(min_nights_binned_indices_train == i)\n",
    "    percentage = (count / len(min_nights_binned_indices_train)) * 100\n",
    "    if i == 0:\n",
    "        label = \"≤3 nights (short-term)\"\n",
    "    elif i == 1:\n",
    "        label = \"4-7 nights (weekly)\"\n",
    "    elif i == 2:\n",
    "        label = \"8-31 nights (monthly)\"\n",
    "    else:\n",
    "        label = \">31 nights (long-term)\"\n",
    "    print(f\"  Bin {i} ({label}): {count} listings ({percentage:.3f}%)\")\n",
    "\n",
    "print(f\"\\nTest set - oh_min_nights shape: {oh_min_nights_test.shape}\")\n",
    "print(\"Distribution:\")\n",
    "for i in range(num_categories):\n",
    "    count = np.sum(min_nights_binned_indices_test == i)\n",
    "    percentage = (count / len(min_nights_binned_indices_test)) * 100\n",
    "    if i == 0:\n",
    "        label = \"≤3 nights (short-term)\"\n",
    "    elif i == 1:\n",
    "        label = \"4-7 nights (weekly)\"\n",
    "    elif i == 2:\n",
    "        label = \"8-31 nights (monthly)\"\n",
    "    else:\n",
    "        label = \">31 nights (long-term)\"\n",
    "    print(f\"  Bin {i} ({label}): {count} listings ({percentage:.3f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e107ab",
   "metadata": {},
   "source": [
    "### 9.3 Log-transform of calc_host_listings_log\n",
    "\n",
    "**Purpose:**\n",
    "- Handle long-tail distribution: Most hosts have only 1 house or few listings, but a few have many (up to hundreds).\n",
    "- Narrow the value range so the model isn't overwhelmed by very large numbers, while maintaining important ranking order.\n",
    "\n",
    "**Technique:**\n",
    "- Log Transformation (log1p): Use function $y = \\ln(x + 1)$.\n",
    "- `log1p` avoids log(0) problem and provides higher accuracy for small values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c6cd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Transform Results for calc_host_listings:\n",
      "\n",
      "Train set:\n",
      "  - Original range: [1.0, 327.0]\n",
      "  - Log-Transformed range: [0.693, 5.793]\n",
      "\n",
      "Test set:\n",
      "  - Original range: [1.0, 327.0]\n",
      "  - Log-Transformed range: [0.693, 5.793]\n"
     ]
    }
   ],
   "source": [
    "# Log-transform for train set\n",
    "calc_host_listings_log_train = np.log1p(calc_host_listings_train)\n",
    "\n",
    "# Log-transform for test set\n",
    "calc_host_listings_log_test = np.log1p(calc_host_listings_test)\n",
    "\n",
    "print(\"Log-Transform Results for calc_host_listings:\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  - Original range: [{calc_host_listings_train.min()}, {calc_host_listings_train.max()}]\")\n",
    "print(f\"  - Log-Transformed range: [{calc_host_listings_log_train.min():.3f}, {calc_host_listings_log_train.max():.3f}]\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - Original range: [{calc_host_listings_test.min()}, {calc_host_listings_test.max()}]\")\n",
    "print(f\"  - Log-Transformed range: [{calc_host_listings_log_test.min():.3f}, {calc_host_listings_log_test.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bf6a2",
   "metadata": {},
   "source": [
    "### 9.4 Log-transform of number_of_reviews_log\n",
    "\n",
    "**Purpose:**\n",
    "- Handle long-tail distribution: Most hosts have only 1 house or few reviews, but a few have many (up to hundreds).\n",
    "- Narrow the value range so the model isn't overwhelmed by very large numbers, while maintaining important ranking order.\n",
    "\n",
    "**Technique:**\n",
    "- Log Transformation (log1p): Use function $y = \\ln(x + 1)$.\n",
    "- `log1p` avoids log(0) problem and provides higher accuracy for small values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f259bdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_reviews transformation:\n",
      "\n",
      "Train set:\n",
      "  - Original range: [0.00, 629.00]\n",
      "  - Log range: [0.000, 6.446]\n",
      "\n",
      "Test set:\n",
      "  - Original range: [0.00, 607.00]\n",
      "  - Log range: [0.000, 6.410]\n"
     ]
    }
   ],
   "source": [
    "# Log-transform for train set\n",
    "number_of_reviews_log_train = np.log1p(number_of_reviews_train)\n",
    "\n",
    "# Log-transform for test set\n",
    "number_of_reviews_log_test = np.log1p(number_of_reviews_test)\n",
    "\n",
    "print(\"number_of_reviews transformation:\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  - Original range: [{number_of_reviews_train.min():.2f}, {number_of_reviews_train.max():.2f}]\")\n",
    "print(f\"  - Log range: [{number_of_reviews_log_train.min():.3f}, {number_of_reviews_log_train.max():.3f}]\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - Original range: [{number_of_reviews_test.min():.2f}, {number_of_reviews_test.max():.2f}]\")\n",
    "print(f\"  - Log range: [{number_of_reviews_log_test.min():.3f}, {number_of_reviews_log_test.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c811a3",
   "metadata": {},
   "source": [
    "### 9.5 Log-transform of price\n",
    "\n",
    "**Purpose:** Reduce price distribution skewness and improve machine learning model performance\n",
    "\n",
    "**Technique:**\n",
    "- Log Transformation (log1p): Use function $y = \\ln(x + 1)$.\n",
    "- `log1p` avoids log(0) problem and provides higher accuracy for small values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed787f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature - Log Price:\n",
      "\n",
      "Train set:\n",
      "  - Original price range: [$10.00, $10000.00]\n",
      "  - Log price range: [2.398, 9.210]\n",
      "\n",
      "Test set:\n",
      "  - Original price range: [$10.00, $10000.00]\n",
      "  - Log price range: [2.398, 9.210]\n"
     ]
    }
   ],
   "source": [
    "# Log-transform for train set\n",
    "price_log_train = np.log1p(prices_train)\n",
    "\n",
    "# Log-transform for test set\n",
    "price_log_test = np.log1p(prices_test)\n",
    "\n",
    "print(\"Feature - Log Price:\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  - Original price range: [${prices_train.min():.2f}, ${prices_train.max():.2f}]\")\n",
    "print(f\"  - Log price range: [{price_log_train.min():.3f}, {price_log_train.max():.3f}]\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - Original price range: [${prices_test.min():.2f}, ${prices_test.max():.2f}]\")\n",
    "print(f\"  - Log price range: [{price_log_test.min():.3f}, {price_log_test.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8df800",
   "metadata": {},
   "source": [
    "## 10. Target Encoding with Smoothing for neighbourhood\n",
    "\n",
    "**Purpose:** Convert categorical variable `neighbourhood` into numeric values based on relationship with target (price)\n",
    "- Target encoding preserves information about the relationship between neighbourhood and price\n",
    "- Smoothing helps avoid overfitting for neighbourhoods with few samples\n",
    "\n",
    "**Technique:** Target Encoding with Smoothing\n",
    "- Formula: `encoded_value = (count * mean + smoothing * global_mean) / (count + smoothing)`\n",
    "- `smoothing`: hyperparameter adjusting confidence (typically 10-100)\n",
    "- Fit on train set, transform both train and test to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628aea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding Results:\n",
      "- neighbourhood: 219 unique values in train\n",
      "\n",
      "Train set:\n",
      "  - Encoded range: [85.537, 386.425]\n",
      "  - Mean: 153.548\n",
      "\n",
      "Test set:\n",
      "  - Encoded range: [85.537, 386.425]\n",
      "  - Mean: 154.793\n",
      "\n",
      "Sample (Original -> Encoded) for train set:\n",
      "  Chelsea -> 242.826\n",
      "  East Harlem -> 127.520\n",
      "  Williamsburg -> 144.999\n",
      "  Washington Heights -> 95.351\n",
      "  Harlem -> 120.122\n"
     ]
    }
   ],
   "source": [
    "def target_encode_with_smoothing(train_cat, train_target, test_cat, smoothing=50):\n",
    "    \"\"\"\n",
    "    Target encoding with smoothing\n",
    "    \n",
    "    Parameters:\n",
    "    - train_cat: categorical values from train set\n",
    "    - train_target: target values from train set (to calculate mean)\n",
    "    - test_cat: categorical values from test set\n",
    "    - smoothing: smoothing parameter (default=50)\n",
    "    \n",
    "    Returns:\n",
    "    - train_encoded: target encoded values for train\n",
    "    - test_encoded: target encoded values for test\n",
    "    \"\"\"\n",
    "    # Calculate global mean from train set\n",
    "    global_mean = np.mean(train_target)\n",
    "    \n",
    "    # Get unique categories from train\n",
    "    unique_cats = np.unique(train_cat)\n",
    "    \n",
    "    # Create dictionary to store encoded values\n",
    "    encoding_map = {}\n",
    "    \n",
    "    for cat in unique_cats:\n",
    "        # Mask for this category in train\n",
    "        mask = (train_cat == cat)\n",
    "        \n",
    "        # Count and calculate mean\n",
    "        count = np.sum(mask)\n",
    "        cat_mean = np.mean(train_target[mask])\n",
    "        \n",
    "        # Apply smoothing\n",
    "        smoothed_value = (count * cat_mean + smoothing * global_mean) / (count + smoothing)\n",
    "        encoding_map[cat] = smoothed_value\n",
    "    \n",
    "    # Encode train set\n",
    "    train_encoded = np.array([encoding_map.get(cat, global_mean) for cat in train_cat])\n",
    "    \n",
    "    # Encode test set (use global_mean for unseen categories)\n",
    "    test_encoded = np.array([encoding_map.get(cat, global_mean) for cat in test_cat])\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Apply target encoding with smoothing for neighbourhood\n",
    "te_neighbourhood_train, te_neighbourhood_test = target_encode_with_smoothing(\n",
    "    neighbourhoods_train, \n",
    "    prices_train,  # Use price as target\n",
    "    neighbourhoods_test,\n",
    "    smoothing=50\n",
    ")\n",
    "\n",
    "print(\"Target Encoding Results:\")\n",
    "print(f\"- neighbourhood: {len(np.unique(neighbourhoods_train))} unique values in train\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  - Encoded range: [{te_neighbourhood_train.min():.3f}, {te_neighbourhood_train.max():.3f}]\")\n",
    "print(f\"  - Mean: {te_neighbourhood_train.mean():.3f}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - Encoded range: [{te_neighbourhood_test.min():.3f}, {te_neighbourhood_test.max():.3f}]\")\n",
    "print(f\"  - Mean: {te_neighbourhood_test.mean():.3f}\")\n",
    "print(f\"\\nSample (Original -> Encoded) for train set:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {neighbourhoods_train[i]} -> {te_neighbourhood_train[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba32dc",
   "metadata": {},
   "source": [
    "## 11. Data normalization\n",
    "\n",
    "**Purpose:** Normalize features to range [0, 1] to ensure features have the same scale\n",
    "\n",
    "**Technique:** Min-Max Scaling\n",
    "- Formula: `(x - min) / (max - min)`\n",
    "- **Important**: Fit on train set (calculate min, max from train), then transform both train and test\n",
    "- Avoid data leakage by not using information from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de19545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaling Results:\n",
      "Normalized 5 features to range [0, 1]\n",
      "\n",
      "Check range of scaled features:\n",
      "\n",
      "Train set:\n",
      "  - number_of_reviews_log: [0.000, 1.000]\n",
      "  - calc_host_listings_log: [0.000, 1.000]\n",
      "  - availability_365: [0.000, 1.000]\n",
      "  - dist_to_center: [0.000, 1.000]\n",
      "  - reviews_per_month: [0.000, 1.000]\n",
      "\n",
      "Test set:\n",
      "  - number_of_reviews_log: [0.000, 0.994]\n",
      "  - calc_host_listings_log: [0.000, 1.000]\n",
      "  - availability_365: [0.000, 1.000]\n",
      "  - dist_to_center: [0.000, 1.010]\n",
      "  - reviews_per_month: [0.000, 0.478]\n",
      "\n",
      "Note: Test set may have values outside [0,1] if outliers not present in train\n"
     ]
    }
   ],
   "source": [
    "def fit_min_max_scale(train_arr):\n",
    "    \"\"\"\n",
    "    Fit min-max scaler on train set\n",
    "    Returns: min_val, max_val, denom\n",
    "    \"\"\"\n",
    "    min_val = np.min(train_arr)\n",
    "    max_val = np.max(train_arr)\n",
    "    denom = max_val - min_val\n",
    "    \n",
    "    if denom == 0:\n",
    "        denom = 1  # Avoid division by zero\n",
    "    \n",
    "    return min_val, max_val, denom\n",
    "\n",
    "def transform_min_max_scale(arr, min_val, max_val, denom):\n",
    "    \"\"\"\n",
    "    Transform data using min, max from train set\n",
    "    \"\"\"\n",
    "    if denom == 0:\n",
    "        return np.zeros_like(arr)\n",
    "    return (arr - min_val) / denom\n",
    "\n",
    "# List of features to scale (already log-transformed)\n",
    "train_features_raw = [\n",
    "    number_of_reviews_log_train,\n",
    "    calc_host_listings_log_train,\n",
    "    availability_train, \n",
    "    dist_to_center_train,\n",
    "    reviews_per_month_train,\n",
    "]\n",
    "\n",
    "test_features_raw = [\n",
    "    number_of_reviews_log_test,\n",
    "    calc_host_listings_log_test,\n",
    "    availability_test, \n",
    "    dist_to_center_test,\n",
    "    reviews_per_month_test,\n",
    "]\n",
    "\n",
    "feature_names = [\n",
    "    'number_of_reviews_log',\n",
    "    'calc_host_listings_log',\n",
    "    'availability_365',\n",
    "    'dist_to_center',\n",
    "    'reviews_per_month',\n",
    "]\n",
    "\n",
    "# Fit on train and transform both train and test\n",
    "train_scaled_features = []\n",
    "test_scaled_features = []\n",
    "scaling_params = []\n",
    "\n",
    "for train_f, test_f in zip(train_features_raw, test_features_raw):\n",
    "    # Fit on train\n",
    "    min_val, max_val, denom = fit_min_max_scale(train_f)\n",
    "    scaling_params.append((min_val, max_val, denom))\n",
    "    \n",
    "    # Transform both train and test\n",
    "    train_scaled = transform_min_max_scale(train_f, min_val, max_val, denom)\n",
    "    test_scaled = transform_min_max_scale(test_f, min_val, max_val, denom)\n",
    "    \n",
    "    train_scaled_features.append(train_scaled)\n",
    "    test_scaled_features.append(test_scaled)\n",
    "\n",
    "print(\"Min-Max Scaling Results:\")\n",
    "print(f\"Normalized {len(train_scaled_features)} features to range [0, 1]\")\n",
    "print(f\"\\nCheck range of scaled features:\")\n",
    "print(\"\\nTrain set:\")\n",
    "for name, scaled_f in zip(feature_names, train_scaled_features):\n",
    "    print(f\"  - {name}: [{scaled_f.min():.3f}, {scaled_f.max():.3f}]\")\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "for name, scaled_f in zip(feature_names, test_scaled_features):\n",
    "    print(f\"  - {name}: [{scaled_f.min():.3f}, {scaled_f.max():.3f}]\")\n",
    "    \n",
    "print(\"\\nNote: Test set may have values outside [0,1] if outliers not present in train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae152fbc",
   "metadata": {},
   "source": [
    "## 12. Array Manipulation - Stacking & Combining features\n",
    "\n",
    "**Purpose:** Combine all processed features into final feature matrix for train and test\n",
    "\n",
    "**Technique:** Stack all features into final matrix\n",
    "- `np.hstack()`: Horizontal stacking\n",
    "- Reshape 1D arrays to (N, 1) before stacking\n",
    "- Feature order: [Target Encoded, One-Hot Encoded, Scaled Numerical Features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d3081f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Matrix:\n",
      "\n",
      "Train set:\n",
      "  - Shape: (39107, 19)\n",
      "  - Total features: 19\n",
      "\n",
      "Test set:\n",
      "  - Shape: (9777, 19)\n",
      "  - Total features: 19\n",
      "\n",
      "Breakdown:\n",
      "  1. Target encoded neighbourhood: 1 feature\n",
      "  2. One-hot room_type: 3 features\n",
      "  3. One-hot neighbourhood_group: 5 features\n",
      "  4. One-hot min_nights: 4 features\n",
      "  5. Scaled numerical features: 5 features\n",
      "  6. Target variable (price_log): 1 feature\n",
      "\n",
      "Data ready for Machine Learning!\n"
     ]
    }
   ],
   "source": [
    "# Reshape 1D arrays to (N, 1) for stacking\n",
    "train_scaled_reshaped = [f[:, None] for f in train_scaled_features]\n",
    "test_scaled_reshaped = [f[:, None] for f in test_scaled_features]\n",
    "\n",
    "# Create final matrix for train set (add price_log at the end)\n",
    "final_matrix_train = np.hstack(\n",
    "    [te_neighbourhood_train[:, None]] +  # Target encoded neighbourhood\n",
    "    [oh_neighbourhood_group_train] +      # One-hot neighbourhood_group\n",
    "    [oh_room_type_train] +                # One-hot room_type\n",
    "    [oh_min_nights_train] +               # One-hot min_nights binned\n",
    "    train_scaled_reshaped +               # Scaled numerical features\n",
    "    [price_log_train[:, None]]            # Target variable (price_log)\n",
    ")\n",
    "\n",
    "# Create final matrix for test set (add price_log at the end)\n",
    "final_matrix_test = np.hstack(\n",
    "    [te_neighbourhood_test[:, None]] +   # Target encoded neighbourhood\n",
    "    [oh_neighbourhood_group_test] +       # One-hot neighbourhood_group\n",
    "    [oh_room_type_test] +                 # One-hot room_type\n",
    "    [oh_min_nights_test] +                # One-hot min_nights binned\n",
    "    test_scaled_reshaped +                # Scaled numerical features\n",
    "    [price_log_test[:, None]]             # Target variable (price_log)\n",
    ")\n",
    "\n",
    "print(\"Final Feature Matrix:\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  - Shape: {final_matrix_train.shape}\")\n",
    "print(f\"  - Total features: {final_matrix_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - Shape: {final_matrix_test.shape}\")\n",
    "print(f\"  - Total features: {final_matrix_test.shape[1]}\")\n",
    "\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  1. Target encoded neighbourhood: 1 feature\")\n",
    "print(f\"  2. One-hot room_type: {oh_room_type_train.shape[1]} features\")\n",
    "print(f\"  3. One-hot neighbourhood_group: {oh_neighbourhood_group_train.shape[1]} features\")\n",
    "print(f\"  4. One-hot min_nights: {oh_min_nights_train.shape[1]} features\")\n",
    "print(f\"  5. Scaled numerical features: {len(train_scaled_features)} features\")\n",
    "print(f\"  6. Target variable (price_log): 1 feature\")\n",
    "print(f\"\\nData ready for Machine Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc5bdc",
   "metadata": {},
   "source": [
    "## 13. Save feature data as CSV\n",
    "\n",
    "**Purpose:** Save processed feature matrix as CSV separately for train and test sets\n",
    "- `train_features.csv`: Train data for model training\n",
    "- `test_features.csv`: Test data for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "603fcd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to: ../data/processed/train_features.csv\n",
      "Shape: (39107, 19)\n",
      "Number of columns: 19\n",
      "Saved data to: ../data/processed/test_features.csv\n",
      "Shape: (9777, 19)\n",
      "Number of columns: 19\n",
      "Saved feature data as CSV:\n",
      "\n",
      "Train set:\n",
      "  - File: ../data/processed/train_features.csv\n",
      "  - Shape: (39107, 19)\n",
      "  - Number of samples: 39,107\n",
      "\n",
      "Test set:\n",
      "  - File: ../data/processed/test_features.csv\n",
      "  - Shape: (9777, 19)\n",
      "  - Number of samples: 9,777\n",
      "\n",
      "Number of features: 19\n",
      "\n",
      "Columns in CSV file:\n",
      "  1. neighbourhood_target_encoded\n",
      "  2. ng_Bronx\n",
      "  3. ng_Brooklyn\n",
      "  4. ng_Manhattan\n",
      "  5. ng_Queens\n",
      "  6. ng_Staten Island\n",
      "  7. room_Entire home/apt\n",
      "  8. room_Private room\n",
      "  9. room_Shared room\n",
      "  10. nights_short_term\n",
      "  11. nights_weekly\n",
      "  12. nights_monthly\n",
      "  13. nights_long_term\n",
      "  14. number_of_reviews_log\n",
      "  15. calc_host_listings_log\n",
      "  16. availability_365\n",
      "  17. dist_to_center\n",
      "  18. reviews_per_month\n",
      "  19. price_log ← Target variable\n",
      "\n",
      "Complete! Data has been split and saved separately to avoid data leakage.\n",
      "Column price_log (target variable) has been added at the end.\n"
     ]
    }
   ],
   "source": [
    "# Create column names for final_matrix\n",
    "feature_column_names = ['neighbourhood_target_encoded']\n",
    "\n",
    "# Add column names for one-hot neighbourhood_group\n",
    "ng_classes = np.unique(neighbourhood_groups_train)\n",
    "feature_column_names.extend([f'ng_{cls}' for cls in ng_classes])\n",
    "\n",
    "\n",
    "# Add column names for one-hot room_type\n",
    "room_type_classes = np.unique(room_types_train)\n",
    "feature_column_names.extend([f'room_{cls}' for cls in room_type_classes])\n",
    "\n",
    "# Add column names for one-hot min_nights\n",
    "min_nights_labels = ['short_term', 'weekly', 'monthly', 'long_term']\n",
    "feature_column_names.extend([f'nights_{label}' for label in min_nights_labels])\n",
    "\n",
    "# Add scaled numerical feature names\n",
    "feature_column_names.extend(feature_names)\n",
    "\n",
    "# Add target variable column (price_log)\n",
    "feature_column_names.append('price_log')\n",
    "\n",
    "# Convert final_matrix to string array for CSV saving\n",
    "final_matrix_train_str = final_matrix_train.astype(str)\n",
    "final_matrix_test_str = final_matrix_test.astype(str)\n",
    "\n",
    "# Save CSV file for train set\n",
    "train_csv_path = '../data/processed/train_features.csv'\n",
    "write_csv(train_csv_path, final_matrix_train_str, np.array(feature_column_names))\n",
    "\n",
    "# Save CSV file for test set\n",
    "test_csv_path = '../data/processed/test_features.csv'\n",
    "write_csv(test_csv_path, final_matrix_test_str, np.array(feature_column_names))\n",
    "\n",
    "print(f\"Saved feature data as CSV:\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  - File: {train_csv_path}\")\n",
    "print(f\"  - Shape: {final_matrix_train.shape}\")\n",
    "print(f\"  - Number of samples: {final_matrix_train.shape[0]:,}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - File: {test_csv_path}\")\n",
    "print(f\"  - Shape: {final_matrix_test.shape}\")\n",
    "print(f\"  - Number of samples: {final_matrix_test.shape[0]:,}\")\n",
    "print(f\"\\nNumber of features: {len(feature_column_names)}\")\n",
    "print(f\"\\nColumns in CSV file:\")\n",
    "for i, col_name in enumerate(feature_column_names):\n",
    "    if i < len(feature_column_names) - 1:\n",
    "        print(f\"  {i+1}. {col_name}\")\n",
    "    else:\n",
    "        print(f\"  {i+1}. {col_name} ← Target variable\")\n",
    "    \n",
    "print(f\"\\nComplete! Data has been split and saved separately to avoid data leakage.\")\n",
    "print(f\"Column price_log (target variable) has been added at the end.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
